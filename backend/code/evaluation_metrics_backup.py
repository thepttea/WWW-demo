# evaluation_metrics.py
# Public Opinion Trend Evaluation Module - Provides LLM-driven evaluation across 9 dimensions
# All scores are generated by the LLM, returning a percentage, summary, and reasoning.

import json
from typing import Dict, Any, List, Tuple
from collections import Counter
from llm_provider import get_llm
from logger import log_message


# ============================================================================
# Data Preparation Functions
# ============================================================================

def prepare_simulation_data(sim: Dict[str, Any]) -> Dict[str, Any]:
    """
    Extracts key information from simulation data for evaluation purposes.
    
    Args:
        sim: The simulation object (from _simulations).
    
    Returns:
        A structured summary of the simulation data.
    """
    agents = sim["agents"]
    discourse_history = sim["discourseHistory"]
    current_round = sim["currentRound"]
    
    # 1. Calculate stance distribution (by round)
    stance_by_round = {}
    for round_num in range(1, current_round + 1):
        round_posts = [post for post in discourse_history 
                      if post[2] == round_num and post[3] is not None]
        if round_posts:
            stance_scores = [post[3] for post in round_posts]
            stance_by_round[round_num] = {
                "average": sum(stance_scores) / len(stance_scores),
                "positive_count": sum(1 for s in stance_scores if s > 0),
                "negative_count": sum(1 for s in stance_scores if s < 0),
                "neutral_count": sum(1 for s in stance_scores if s == 0),
                "total": len(stance_scores),
                "extreme_positive": sum(1 for s in stance_scores if s >= 2),
                "extreme_negative": sum(1 for s in stance_scores if s <= -2)
            }
    
    # 2. Extract representative posts (by stance category)
    representative_posts = _extract_representative_posts(discourse_history, agents, limit=30)
    
    # 3. Build full discourse history text
    formatted_history = _format_discourse_history(discourse_history, agents)
    
    # 4. Gather PR strategy information
    pr_strategies = []
    for author_id, content, round_num, stance in discourse_history:
        if author_id == "system" and round_num > 0:
            pr_strategies.append({
                "round": round_num,
                "content": content
            })
    
    # Calculate overall trend metrics
    if len(stance_by_round) >= 2:
        first_round_avg = stance_by_round[min(stance_by_round.keys())]["average"]
        last_round_avg = stance_by_round[current_round]["average"]
        overall_trend = "Improving" if last_round_avg > first_round_avg else ("Worsening" if last_round_avg < first_round_avg else "Stable")
        trend_magnitude = abs(last_round_avg - first_round_avg)
    else:
        overall_trend = "Insufficient Data"
        trend_magnitude = 0
    
    # Calculate engagement metrics
    total_user_posts = len([p for p in discourse_history if p[0] != "system"])
    posts_per_round = total_user_posts / current_round if current_round > 0 else 0
    
    return {
        "simulation_id": sim["simulationId"],
        "scenario": sim.get("scenario", "unknown"),
        "total_rounds": current_round,
        "initial_topic": sim.get("initialTopic", ""),
        "stance_by_round": stance_by_round,
        "representative_posts": representative_posts,
        "pr_strategies": pr_strategies,
        "formatted_history": formatted_history,
        "total_agents": len(agents),
        "total_posts": total_user_posts,
        "posts_per_round": round(posts_per_round, 1),
        "overall_trend": overall_trend,
        "trend_magnitude": round(trend_magnitude, 2)
    }


def prepare_real_case_context(case: Dict[str, Any]) -> Dict[str, Any]:
    """
    Extracts key descriptive information from a real case.
    
    Args:
        case: The case object (from historical_cases.json, normalized by case_manager).
    
    Returns:
        A structured summary of the real case.
    """
    # The data has been normalized by case_manager, so standard fields can be used directly.
    strategies = [
        {
            "round": s.get("round", s.get("node_id", 0)),
            "title": s.get("title", s.get("strategy", "")),
            "content": s.get("content", ""),
            "timeline": s.get("timeline", s.get("timestamp", ""))
        }
        for s in case.get("strategies", [])
    ]
    
    # Extract public opinion description from the real case (if available).
    # Extract situation information from nodes.
    situation_summaries = []
    if "nodes" in case:
        for node in case["nodes"]:
            if "situation" in node:
                sit = node["situation"]
                round_num = node.get("node_id", 0)
                situation_summaries.append({
                    "round": round_num,
                    "overall_stance": sit.get("overall_stance", ""),
                    "evolution_direction": sit.get("evolution_direction", ""),
                    "polarization_level": sit.get("polarization_level", ""),
                    "core_dispute": sit.get("core_dispute", ""),
                    "main_arguments": sit.get("main_arguments", ""),
                    "emotional_tone": sit.get("emotional_tone", ""),
                    "secondary_topic_diffusion": sit.get("secondary_topic_diffusion", "")
                })
    
    return {
        "case_id": case.get("id", case.get("event_id", "")),
        "title": case.get("title", case.get("event_name", "")),
        "background": case.get("background", case.get("event_summary", "")),
        "industry": case.get("industry", case.get("crisis_type", "")),
        "company": case.get("company", ""),
        "date": case.get("date", ""),
        "crisis_type": case.get("crisis_type", ""),
        "core_conflict": case.get("core_conflict", ""),
        "total_rounds": case.get("totalRounds", len(strategies)),
        "strategies": strategies,
        "outcome": case["realWorldOutcome"],
        "key_factors": case["realWorldOutcome"].get("keyFactors", []),
        "situation_summaries": situation_summaries  # New: Public opinion description for each round.
    }


# ============================================================================
# Helper Functions
# ============================================================================

def _get_similarity_context_note() -> str:
    """Gets the context note for similarity evaluation (general)."""
    return """[Important Evaluation Note]
- The simulation only executes partial rounds of the real case, representing an initial-stage replication of the real event.
- The evaluation should focus on the similarity of the "executed stage," not on overall completeness.
- If the simulation exhibits features similar to the real case at the corresponding stage, it should be given a high similarity score.
- The score should not be lowered due to the "simulation not being comprehensive enough"; the focus is on the consistency of "direction" and "trend." """

def _get_similarity_evaluation_criteria(dimension_name: str) -> str:
    """Gets detailed criteria for similarity evaluation."""
    criteria = {
        "Stance Tendency": """
[Scoring Criteria]
90-100%: Stance tendency is completely consistent (e.g., both support/oppose/neutral), and the intensity is similar (difference < 15%).
75-89%: Stance tendency is consistent, but there is some difference in intensity (difference 15-30%).
60-74%: Stance tendency is basically consistent, but the intensity difference is large (difference 30-45%).
40-59%: Stance tendency is partially consistent, or the direction is the same but the intensity difference is very large.
20-39%: Stance tendency is inconsistent, with directional differences.
0-19%: Stance tendency is completely opposite.

[Key Focus]
- Primarily, whether the stance "direction" (support/oppose/neutral) is consistent.
- Secondly, the proximity of stance "intensity".
- The simulation only needs to exhibit similar features in the corresponding rounds.
        """,
        
        "Evolution Direction": """
[Scoring Criteria]
90-100%: Evolution direction is completely consistent (both improving/worsening/stable), and the trend slope is similar.
75-89%: Evolution direction is consistent, but there are differences in trend intensity.
60-74%: Evolution direction is basically consistent, with some local fluctuation differences.
40-59%: Evolution direction is partially consistent, or there are similar turning points.
20-39%: Evolution direction is not very consistent, with clear differences.
0-19%: Evolution direction is completely opposite.

[Key Focus]
- Primarily, whether the overall trend (improving/worsening/stable) is consistent.
- Secondly, whether key turning points occur at similar times.
- Local fluctuation differences are allowed; focus on the major trend.
        """,
        
        "Degree of Polarization": """
[Scoring Criteria]
90-100%: Degree of polarization is very similar (both highly polarized/moderately polarized/basically consistent).
75-89%: Degree of polarization is similar, but there are differences in specific values.
60-74%: Degree of polarization is basically at the same level, but the features are not completely identical.
40-59%: There is some difference in the degree of polarization, but not completely opposite.
20-39%: The degree of polarization differs significantly.
0-19%: The degree of polarization is completely opposite (one is highly polarized, the other is completely uniform).

[Key Focus]
- Primarily, whether the polarization "level" (high/medium/low) is consistent.
- Secondly, the distribution characteristics of extreme viewpoints.
- Reasonable differences in specific proportions are allowed.
        """,
        
        "Turning Point Timing": """
[Scoring Criteria]
90-100%: The turning point occurs in the same round, and both direction and intensity are similar.
75-89%: The turning point timing is close (within 1 round), and the direction is consistent.
60-74%: A turning point exists and the direction is consistent, but the timing differs to some extent.
40-59%: Some turning points are similar, or the turning features partially match.
20-39%: The timing or features of the turning point differ significantly.
0-19%: There are no similar turning points at all.

[Key Focus]
- Primarily, whether a turning point occurs in a similar round.
- Secondly, whether the direction of the turning point (improving/worsening) is consistent.
- If the simulation has few rounds, only compare the turning points of the executed rounds.
        """,
        
        "Core Issue Focus": """
[Scoring Criteria]
90-100%: The focus of discussion is highly consistent, and the core issues completely overlap.
75-89%: The focus of discussion is similar, with a high degree of overlap in major issues.
60-74%: The focus of discussion is basically consistent, with some differences in minor issues.
40-59%: Core issues partially overlap, but there are also clear differences.
20-39%: The focus of discussion differs significantly, with low overlap.
0-19%: The focus of discussion is completely different.

[Key Focus]
- Primarily, whether the core points of contention are the same.
- Secondly, the coverage of the main topics of discussion.
- Differences in minor issues are allowed.
        """,
        
        "Mainstream Arguments": """
[Scoring Criteria]
90-100%: Mainstream arguments are highly consistent; the core viewpoints of both supporters and opponents are similar.
75-89%: Mainstream arguments are similar, and most key arguments correspond.
60-74%: Mainstream arguments are basically consistent, with some differences in arguments.
40-59%: Some mainstream arguments are similar, but there are also clear differences.
20-39%: Mainstream arguments differ significantly.
0-19%: Mainstream arguments are completely different.

[Key Focus]
- Primarily, whether the core arguments of the supporting and opposing sides are similar.
- Secondly, the logic and direction of the arguments.
- Differences in specific wording and details are allowed.
        """,
        
        "Emotional Tone": """
[Scoring Criteria]
90-100%: The emotional tone is highly consistent (e.g., both angry/calm/sad), and the intensity is similar.
75-89%: The emotional tone is consistent, but there is some difference in intensity.
60-74%: The emotional tone is basically consistent, with local differences.
40-59%: The emotional tone is partially similar, but there are also clear differences.
20-39%: The emotional tone differs significantly.
0-19%: The emotional tone is completely opposite (e.g., one is angry, the other is peaceful).

[Key Focus]
- Primarily, whether the dominant emotion type is consistent.
- Secondly, the level of emotional intensity (high/medium/low).
- Differences in the expression of emotions are allowed.
        """,
        
        "PR Response": """
[Scoring Criteria]
90-100%: The response pattern to the PR strategy is highly consistent, and the effect is similar.
75-89%: The response pattern is similar, and the overall effect is close.
60-74%: The response pattern is basically consistent, with local differences in effect.
40-59%: Some response features are similar, but the overall effect differs.
20-39%: The response pattern differs significantly.
0-19%: The response pattern is completely different.

[Key Focus]
- Primarily, whether public acceptance of the strategy is similar.
- Secondly, the direction of the effect after strategy implementation (positive/negative).
- Reasonable differences in specific values are allowed.
        """,
        
        "Topic Diffusion": """
[Scoring Criteria]
90-100%: The topic diffusion path is highly consistent, and secondary topics are similar.
75-89%: The diffusion path is similar, and major secondary topics correspond.
60-74%: The diffusion path is basically consistent, with some differences in secondary topics.
40-59%: Some diffusion features are similar, but the paths differ.
20-39%: The diffusion path differs significantly.
0-19%: The diffusion path is completely different.

[Key Focus]
- Primarily, whether similar secondary topics were generated.
- Secondly, the main direction of topic diffusion.
- Differences in diffusion details and specific paths are allowed.
        """
    }
    
    # Match criteria based on dimension name
    for key, value in criteria.items():
        if key in dimension_name:
            return value
    
    # Default General Criteria
    return """
[Scoring Criteria]
90-100%: Highly similar, core features are completely consistent.
75-89%: High similarity, main features are consistent, with minor differences.
60-74%: Basically similar, core features are consistent, with clear differences.
40-59%: Partially similar, with both commonalities and differences.
20-39%: Low similarity, differences are obvious.
0-19%: Almost completely different.

[Key Focus]
- Focus on the corresponding features of the executed rounds.
- Focus on direction and trend rather than absolute values.
- Allow for reasonable differences in a form of expression.
    """

def _extract_representative_posts(discourse_history: List[Tuple], agents: Dict, limit: int = 30) -> Dict[str, List]:
    """Extracts representative posts (categorized by stance)."""
    posts_by_stance = {
        "strong_support": [],  # stance >= 2
        "support": [],         # 1 <= stance < 2
        "neutral": [],         # stance == 0
        "oppose": [],          # -2 < stance <= -1
        "strong_oppose": []    # stance <= -2
    }
    
    for author_id, content, round_num, stance in discourse_history:
        if author_id == "system" or stance is None:
            continue
            
        agent = agents.get(author_id)
        username = agent.persona.get("username") if agent else "Unknown"
        influence = agent.persona.get("influence_score", 50) if agent else 50
        
        post_data = {
            "username": username,
            "content": content,
            "stance": stance,
            "influence": influence,
            "round": round_num
        }
        
        if stance >= 2:
            posts_by_stance["strong_support"].append(post_data)
        elif stance >= 1:
            posts_by_stance["support"].append(post_data)
        elif stance <= -2:
            posts_by_stance["strong_oppose"].append(post_data)
        elif stance <= -1:
            posts_by_stance["oppose"].append(post_data)
        else:
            posts_by_stance["neutral"].append(post_data)
    
    # Select high-influence posts from each category
    result = {}
    for category, posts in posts_by_stance.items():
        sorted_posts = sorted(posts, key=lambda x: x["influence"], reverse=True)
        result[category] = sorted_posts[:10]  # Max 10 per category
    
    return result


def _format_discourse_history(discourse_history: List[Tuple], agents: Dict) -> str:
    """Formats the discourse history into a text string."""
    formatted = ""
    for author_id, content, round_num, stance in discourse_history:
        if author_id == "system":
            formatted += f"\n[Round {round_num}] [Official Statement]: {content}\n"
        else:
            username = agents[author_id].persona.get('username', author_id) if author_id in agents else "Unknown"
            formatted += f"[Round {round_num}] {username} (Stance:{stance}): {content}\n"
    return formatted


def _parse_llm_json_response(response: str) -> Dict:
    """Parses the JSON response from the LLM."""
    try:
        # Attempt to parse directly
        return json.loads(response)
    except json.JSONDecodeError:
        # Attempt to extract the JSON part
        import re
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group())
            except:
                pass
        
        # If it fails, return a default structure
        log_message(f"WARNING: Failed to parse LLM response as JSON: {response[:200]}...")
        return {
            "percentage": 50,
            "summary": "Parsing failed",
            "reasoning": "LLM response format error",
            "raw_response": response
        }


# ============================================================================
# LLM Evaluation Functions - 9 Dimensions (All Evaluated by LLM)
# ============================================================================

def evaluate_stance_tendency(sim_data: Dict, real_case_data: Dict = None) -> Dict[str, Any]:
    """
    Dimension 1: Overall Stance Tendency
    Use LLM to summarize stance features, without numerical scoring.
    """
    log_message("Evaluating Dimension 1: Overall Stance Tendency")
    
    llm = get_llm()
    
    # Prepare stance statistics
    last_round = sim_data["total_rounds"]
    stance_stats = sim_data["stance_by_round"].get(last_round, {})
    
    posts_sample = ""
    for category, posts in sim_data["representative_posts"].items():
        if posts:
            posts_sample += f"\n[{category} Posts]:\n"
            for p in posts[:3]:
                posts_sample += f"- {p['username']}: {p['content'][:60]}...\n"
    
    # Scenario 1: Evaluate simulation only
    if not real_case_data:
        prompt = f"""Please summarize the overall stance tendency features of the following simulation.

Simulation Data Statistics:
- Support Count: {stance_stats.get('positive_count', 0)}/{stance_stats.get('total', 0)}
- Oppose Count: {stance_stats.get('negative_count', 0)}/{stance_stats.get('total', 0)}
- Neutral Count: {stance_stats.get('neutral_count', 0)}/{stance_stats.get('total', 0)}
- Average Stance: {stance_stats.get('average', 0):.2f} (Range -3 to +3)

Representative Post Samples:
{posts_sample}

Please provide a detailed description of the stance tendency features in 200-300 words, including:
1. Main stance direction (Support/Oppose/Neutral)
2. Stance intensity (Strong/Moderate/Slight)
3. Stance distribution characteristics
4. Typical viewpoints and attitudes

Please return in JSON format:
{{
    "summary": "A detailed summary of stance tendency (200-300 words)"
}}"""
        
        response = llm.invoke(prompt).content
        result = _parse_llm_json_response(response)
        result["category"] = "simulation_only"
        return result
    
    # Scenario 2: Summarize features of simulation and real case separately, then have LLM evaluate similarity
    else:
        # Step 1: Summarize simulation situation
        sim_prompt = f"""Please provide a detailed summary of the overall stance tendency features of the following simulation.

Simulation Data Statistics:
- Support Count: {stance_stats.get('positive_count', 0)}/{stance_stats.get('total', 0)}
- Oppose Count: {stance_stats.get('negative_count', 0)}/{stance_stats.get('total', 0)}
- Neutral Count: {stance_stats.get('neutral_count', 0)}/{stance_stats.get('total', 0)}
- Average Stance: {stance_stats.get('average', 0):.2f}

Representative Post Samples:
{posts_sample[:1500]}

Please provide a detailed description of the stance tendency in 200-300 words, including: main stance direction, stance intensity, distribution characteristics, and typical attitudes.

Return in JSON format: {{"summary": "Detailed description (200-300 words)"}}"""
        
        sim_result = _parse_llm_json_response(llm.invoke(sim_prompt).content)
        
        # Step 2: Summarize real case situation
        stance_info = ""
        if real_case_data.get("situation_summaries"):
            for sit_sum in real_case_data["situation_summaries"]:
                if sit_sum.get("overall_stance"):
                    stance_info += f"\nRound {sit_sum['round']}: {sit_sum['overall_stance']}"
        
        real_prompt = f"""Please provide a detailed summary of the overall stance tendency features of the real case.

Case: {real_case_data['title']}
Background: {real_case_data['background']}
Core Conflict: {real_case_data.get('core_conflict', '')}

{'[Public Opinion Stance by Round]' + stance_info if stance_info else ''}

Final Outcome: {json.dumps(real_case_data['outcome'], ensure_ascii=False)}

Please provide a detailed description of the stance tendency in 200-300 words, including: main stance direction, stance intensity, distribution characteristics, and typical attitudes.

Return in JSON format: {{"summary": "Detailed description (200-300 words)"}}"""
        
        real_result = _parse_llm_json_response(llm.invoke(real_prompt).content)
        
        # Step 3: LLM directly compares the two descriptions to evaluate similarity
        context_note = _get_similarity_context_note()
        
        similarity_prompt = f"""Please evaluate the similarity between the simulation and the real case in the "Overall Stance Tendency" dimension.

{context_note}

[Simulation's Stance Tendency Features]
{sim_result['summary']}

[Real Case's Stance Tendency Features]
{real_result['summary']}

Evaluation Requirements:
1. Carefully compare the core features of the two descriptions.
2. Focus mainly on: whether the stance direction is consistent, whether the stance intensity is close, and whether the overall attitude is similar.
3. The simulation only needs to exhibit similar features in the executed rounds; it is not required to fully cover all details of the real case.
4. If the core direction is consistent and the main features are similar, a higher score (above 75%) should be given.
5. The score can be a decimal (e.g., 85.5%).

Scoring Reference (for reference only, can be adjusted based on the actual situation):
- 90-100%: Stance direction, intensity, and attitude are highly consistent.
- 80-89%: Core features are consistent, with differences in details.
- 70-79%: Main features are similar, with differences in some aspects.
- 60-69%: Basically similar, but with clear differences.
- 50-59%: Partially similar, with significant differences.
- <50%: Differences are obvious or directions are opposite.

Please return in JSON format (return the number directly, do not explain):
{{
    "similarity_score": "number (0-100, can be a decimal like 85.5)",
    "reasoning": "Detailed explanation of why this similarity score was given, including: 1) core feature comparison 2) similarities 3) differences 4) comprehensive scoring basis (200-300 words)"
}}"""
        
        similarity_result = _parse_llm_json_response(llm.invoke(similarity_prompt).content)
        
        return {
            "category": "comparative",
            "simulation": sim_result,
            "real_case": real_result,
            "similarity": similarity_result
        }


def evaluate_evolution_direction(sim_data: Dict, real_case_data: Dict = None) -> Dict[str, Any]:
    """
    Dimension 2: Public Opinion Evolution Direction
    Use LLM to evaluate evolution trends.
    """
    log_message("Evaluating Dimension 2: Public Opinion Evolution Direction")
    
    llm = get_llm()
    
    # Build evolution data
    evolution_text = ""
    for round_num in sorted(sim_data["stance_by_round"].keys()):
        stats = sim_data["stance_by_round"][round_num]
        avg = stats["average"]
        evolution_text += f"Round {round_num}: Average Stance {avg:.2f}, Support {stats['positive_count']}/Oppose {stats['negative_count']}/Neutral {stats['neutral_count']}\n"
    
    if not real_case_data:
        prompt = f"""Please evaluate the following public opinion evolution direction.

Evolution Data:
{evolution_text}

PR Strategies:
{json.dumps(sim_data['pr_strategies'], ensure_ascii=False, indent=2)}

Please return in JSON format:
{{
    "percentage": "number (0-100, 50=stable, >50=improving, <50=worsening)",
    "summary": "e.g., 'Evolution trend 70%, public opinion gradually improving'",
    "reasoning": "Reasoning for the score"
}}"""
        
        response = llm.invoke(prompt).content
        result = _parse_llm_json_response(response)
        result["category"] = "simulation_only"
        return result
    
    else:
        # Evaluate simulation
        sim_prompt = f"""Please evaluate the public opinion evolution direction of the simulation.

Evolution Data:
{evolution_text}

Please return in JSON format:
{{
    "percentage": "number (0-100)",
    "summary": "Evolution summary",
    "reasoning": "Reasoning"
}}"""
        
        sim_result = _parse_llm_json_response(llm.invoke(sim_prompt).content)
        
        # Evaluate real case
        strategies_text = "\n".join([
            f"Round {s['round']}: {s['title']} - {s['content'][:100]}..."
            for s in real_case_data['strategies']
        ])
        
        # Extract evolution direction information
        evolution_info = ""
        if real_case_data.get("situation_summaries"):
            for sit_sum in real_case_data["situation_summaries"]:
                if sit_sum.get("evolution_direction"):
                    evolution_info += f"\nRound {sit_sum['round']}: {sit_sum['evolution_direction']}"
        
        real_prompt = f"""Please evaluate the public opinion evolution direction of the real case.

Case: {real_case_data['title']}
Strategies by Round:
{strategies_text}

{'[Public Opinion Evolution by Round]' + evolution_info if evolution_info else ''}

Outcome: {real_case_data['outcome']['success']}
Description: {json.dumps(real_case_data['outcome']['metrics'], ensure_ascii=False)}

Please return in JSON format:
{{
    "percentage": "number (0-100, 50=stable, >50=improving, <50=worsening)",
    "summary": "Evolution summary, e.g., 'Evolution trend 35%, continuously worsening'",
    "reasoning": "Reasoning based on the evolution description of each round"
}}"""
        
        real_result = _parse_llm_json_response(llm.invoke(real_prompt).content)
        
        # Evaluate similarity
        context_note = _get_similarity_context_note()
        criteria = _get_similarity_evaluation_criteria("Public Opinion Evolution Direction")
        
        similarity_prompt = f"""Compare the similarity of public opinion evolution directions.

{context_note}

Simulation Situation: {sim_result['summary']} ({sim_result['percentage']}%)
Real Case: {real_result['summary']} ({real_result['percentage']}%)

{criteria}

Evaluation Steps:
1. Judge if the evolution "direction" is consistent (improving/worsening/stable).
2. Compare if the evolution "trend intensity" is close.
3. Provide a similarity score based on the scoring criteria.
4. Explain the basis for the score.

Please return in JSON format:
{{
    "similarity_percentage": "number (0-100, strictly according to the scoring criteria above)",
    "summary": "Similarity summary, e.g., 'Similarity 82%, both show a gradually improving trend'",
    "reasoning": "Detailed explanation: 1) direction comparison 2) trend intensity comparison 3) why this similarity score was given"
}}"""
        
        similarity_result = _parse_llm_json_response(llm.invoke(similarity_prompt).content)
        
        return {
            "category": "comparative",
            "simulation": sim_result,
            "real_case": real_result,
            "similarity": similarity_result
        }


def evaluate_polarization_degree(sim_data: Dict, real_case_data: Dict = None) -> Dict[str, Any]:
    """
    Dimension 3: Degree of Public Opinion Polarization
    """
    log_message("Evaluating Dimension 3: Degree of Public Opinion Polarization")
    
    llm = get_llm()
    
    last_round = sim_data["total_rounds"]
    stats = sim_data["stance_by_round"].get(last_round, {})
    
    extreme_info = f"Extreme Support: {stats.get('extreme_positive', 0)}, Extreme Oppose: {stats.get('extreme_negative', 0)}, Total: {stats.get('total', 1)}"
    
    if not real_case_data:
        prompt = f"""Please evaluate the degree of public opinion polarization.

Statistical Data: {extreme_info}
Support/Oppose/Neutral: {stats.get('positive_count', 0)}/{stats.get('negative_count', 0)}/{stats.get('neutral_count', 0)}

Please return in JSON format:
{{
    "percentage": "number (0-100, indicating degree of polarization, 0=completely uniform, 100=severely bipolar)",
    "summary": "e.g., 'Polarization degree 65%, opinions are clearly divided'",
    "reasoning": "Reasoning"
}}"""
        
        result = _parse_llm_json_response(llm.invoke(prompt).content)
        result["category"] = "simulation_only"
        return result
    
    else:
        sim_result = _parse_llm_json_response(llm.invoke(f"""Evaluate the degree of polarization in the simulation.
Data: {extreme_info}
Return JSON: {{"percentage": "number", "summary": "Summary", "reasoning": "Reasoning"}}""").content)
        
        # Extract polarization degree information
        polarization_info = ""
        if real_case_data.get("situation_summaries"):
            for sit_sum in real_case_data["situation_summaries"]:
                if sit_sum.get("polarization_level"):
                    polarization_info += f"\nRound {sit_sum['round']}: {sit_sum['polarization_level']}"
        
        real_result = _parse_llm_json_response(llm.invoke(f"""Evaluate the degree of polarization in the real case.
Case: {real_case_data['title']}
{'[Polarization Degree by Round]' + polarization_info if polarization_info else ''}
Final Description: {real_case_data['outcome']}
Return JSON: {{"percentage": "number (0-100, 0=completely uniform, 100=severely polarized)", "summary": "Summary", "reasoning": "Reasoning"}}""").content)
        
        context_note = _get_similarity_context_note()
        criteria = _get_similarity_evaluation_criteria("Degree of Public Opinion Polarization")
        
        similarity_result = _parse_llm_json_response(llm.invoke(f"""Compare the similarity of polarization degrees.

{context_note}

Simulation: {sim_result['summary']} ({sim_result['percentage']}%)
Real Case: {real_result['summary']} ({real_result['percentage']}%)

{criteria}

Evaluation Steps:
1. Judge if the polarization "level" (high/medium/low) is consistent.
2. Compare the distribution features of extreme viewpoints.
3. Provide a similarity score based on the scoring criteria.

Return JSON: {{
    "similarity_percentage": "number (0-100, as per scoring criteria)", 
    "summary": "Summary, e.g., 'Similarity 75%, both show moderate polarization'", 
    "reasoning": "Explanation: 1) level comparison 2) feature comparison 3) basis for scoring"
}}""").content)
        
        return {
            "category": "comparative",
            "simulation": sim_result,
            "real_case": real_result,
            "similarity": similarity_result
        }


def evaluate_turning_point_timing(sim_data: Dict, real_case_data: Dict = None) -> Dict[str, Any]:
    """
    Dimension 4: Timing of Key Turning Points
    """
    log_message("Evaluating Dimension 4: Timing of Key Turning Points")
    
    llm = get_llm()
    
    # Analyze turning points - provide more detailed data
    turning_analysis = ""
    turning_points = []
    prev_avg = None
    
    for round_num in sorted(sim_data["stance_by_round"].keys()):
        stats = sim_data["stance_by_round"][round_num]
        current_avg = stats["average"]
        
        turning_analysis += f"Round {round_num}: Average Stance {current_avg:.2f}, "
        turning_analysis += f"Support {stats['positive_count']}/Oppose {stats['negative_count']}/Neutral {stats['neutral_count']}\n"
        
        if prev_avg is not None:
            change = current_avg - prev_avg
            if abs(change) > 0.5:  # Significant change
                turning_points.append({
                    "round": round_num,
                    "change": change,
                    "direction": "Improving" if change > 0 else "Worsening"
                })
                turning_analysis += f"  ⚠️ Turning Point: Stance change {change:+.2f} ({'Improving' if change > 0 else 'Worsening'})\n"
        prev_avg = current_avg
    
    has_turning_point = len(turning_points) > 0
    pr_count = len(sim_data['pr_strategies'])
    
    if not real_case_data:
        prompt = f"""Please evaluate the reasonableness of the turning point timing.

Detailed Evolution:
{turning_analysis}

Detected Turning Points:
{json.dumps(turning_points, ensure_ascii=False) if turning_points else 'No significant turning points detected'}

Number of PR strategy rounds: {pr_count}

Evaluation Criteria:
- `percentage` represents the reasonableness of the turning points (0-100).
- If there is a turning point and it is related to a PR strategy, the percentage should be high (70-90).
- If there is no turning point but there are PR strategies, the percentage should be medium (40-60).
- If the turning point is unrelated to PR strategies, the percentage should be low (20-40).

Please return in JSON format (ensure `percentage` is between 0-100):
{{
    "percentage": "number (0-100)",
    "summary": "e.g., 'Turning point occurred 75%, clear improvement after round 2 strategy' or 'Turning point missing 40%, no clear shift in public opinion'",
    "reasoning": "Reasoning for the score, explaining the timing and reasonableness of the turning points"
}}"""
        
        response = llm.invoke(prompt).content
        result = _parse_llm_json_response(response)
        
        # Validate and correct percentage
        if 'percentage' in result:
            result['percentage'] = max(0, min(100, abs(result['percentage'])))
        else:
            result['percentage'] = 50
        
        result["category"] = "simulation_only"
        return result
    
    else:
        # Evaluate simulation
        sim_prompt = f"""Evaluate the timing of the simulation's turning points.

Evolution Situation:
{turning_analysis}

Turning Points: {json.dumps(turning_points, ensure_ascii=False) if turning_points else 'No clear turning points'}

Please return in JSON format (percentage must be between 0-100):
{{
    "percentage": "number (0-100)",
    "summary": "Turning point summary",
    "reasoning": "Reasoning"
}}"""
        
        sim_response = llm.invoke(sim_prompt).content
        sim_result = _parse_llm_json_response(sim_response)
        sim_result['percentage'] = max(0, min(100, abs(sim_result.get('percentage', 50))))
        
        # Evaluate real case
        strategies_desc = "\n".join([
            f"Round {s['round']}: {s['title']}"
            for s in real_case_data['strategies']
        ])
        
        real_prompt = f"""Evaluate the timing of the real case's turning points.

Case: {real_case_data['title']}

Strategies by Round:
{strategies_desc}

Final Outcome: {'Success' if real_case_data['outcome']['success'] else 'Failure'}
Key Factors: {', '.join(real_case_data['key_factors'])}

Please return in JSON format (percentage must be between 0-100):
{{
    "percentage": "number (0-100)",
    "summary": "Turning point summary, e.g., 'Round 2 strategy took effect 70%'",
    "reasoning": "Reasoning"
}}"""
        
        real_response = llm.invoke(real_prompt).content
        real_result = _parse_llm_json_response(real_response)
        real_result['percentage'] = max(0, min(100, abs(real_result.get('percentage', 50))))
        
        # Evaluate similarity
        context_note = _get_similarity_context_note()
        criteria = _get_similarity_evaluation_criteria("Timing of Key Turning Points")
        
        similarity_prompt = f"""Compare the similarity of turning point timings.

{context_note}

Simulation Turning Points: {sim_result['summary']} ({sim_result['percentage']}%)
Real Case Turning Points: {real_result['summary']} ({real_result['percentage']}%)

{criteria}

Evaluation Steps:
1. Compare if the "rounds" of turning point occurrences are close.
2. Compare if the "direction" of the turns is consistent.
3. Provide a similarity score based on the scoring criteria.

Please return in JSON format (similarity_percentage must be between 0-100):
{{
    "similarity_percentage": "number (0-100, as per scoring criteria)",
    "summary": "Similarity summary, e.g., 'Similarity 88%, both improved in round 2'",
    "reasoning": "Explanation: 1) round comparison 2) direction comparison 3) basis for scoring"
}}"""
        
        similarity_response = llm.invoke(similarity_prompt).content
        similarity_result = _parse_llm_json_response(similarity_response)
        similarity_result['similarity_percentage'] = max(0, min(100, abs(similarity_result.get('similarity_percentage', 50))))
        
        return {
            "category": "comparative",
            "simulation": sim_result,
            "real_case": real_result,
            "similarity": similarity_result
        }


def evaluate_core_issue_focus(sim_data: Dict, real_case_data: Dict = None) -> Dict[str, Any]:
    """
    Dimension 5: Core Issue Focus
    """
    log_message("Evaluating Dimension 5: Core Issue Focus")
    
    llm = get_llm()
    
    posts_sample = sim_data["formatted_history"][:2000]
    
    if not real_case_data:
        prompt = f"""Please evaluate the core focus of the discussion.

Topic: {sim_data['initial_topic']}

Discussion Sample:
{posts_sample}

Please return in JSON format:
{{
    "percentage": "number (0-100, indicating focus concentration)",
    "summary": "e.g., 'Focus concentration 80%, discussion mainly about privacy issues'",
    "reasoning": "Reasoning"
}}"""
        
        result = _parse_llm_json_response(llm.invoke(prompt).content)
        result["category"] = "simulation_only"
        return result
    
    else:
        sim_result = _parse_llm_json_response(llm.invoke(f"""Evaluate simulation focus.
Topic: {sim_data['initial_topic']}
Discussion: {posts_sample[:1000]}
Return JSON: {{"percentage": "number", "summary": "Summary", "reasoning": "Reasoning"}}""").content)
        
        # Extract core dispute information
        dispute_info = ""
        if real_case_data.get("situation_summaries"):
            for sit_sum in real_case_data["situation_summaries"]:
                if sit_sum.get("core_dispute"):
                    dispute_info += f"\nRound {sit_sum['round']}: {sit_sum['core_dispute']}"
        
        real_result = _parse_llm_json_response(llm.invoke(f"""Evaluate real case focus.
Case: {real_case_data['title']}
Background: {real_case_data['background']}
Core Conflict: {real_case_data.get('core_conflict', '')}
{'[Core Dispute by Round]' + dispute_info if dispute_info else ''}
Key Factors: {', '.join(real_case_data['key_factors'])}
Return JSON: {{"percentage": "number (0-100, indicates focus concentration)", "summary": "Summary", "reasoning": "Reasoning"}}""").content)
        
        context_note = _get_similarity_context_note()
        criteria = _get_similarity_evaluation_criteria("Core Issue Focus")
        
        similarity_result = _parse_llm_json_response(llm.invoke(f"""Compare focus similarity.

{context_note}

Simulation: {sim_result['summary']} ({sim_result['percentage']}%)
Real Case: {real_result['summary']} ({real_result['percentage']}%)

{criteria}

Evaluation Steps:
1. Identify and compare the core points of dispute.
2. Judge the degree of overlap of major issues.
3. Give a similarity score based on the scoring criteria.

Return JSON: {{
    "similarity_percentage": "number (0-100, as per scoring criteria)", 
    "summary": "Summary, e.g., 'Similarity 85%, both focused on privacy issues'", 
    "reasoning": "Explanation: 1) core issue comparison 2) coverage analysis 3) basis for scoring"
}}""").content)
        
        return {
            "category": "comparative",
            "simulation": sim_result,
            "real_case": real_result,
            "similarity": similarity_result
        }


def evaluate_mainstream_arguments(sim_data: Dict, real_case_data: Dict = None) -> Dict[str, Any]:
    """
    Dimension 6: Mainstream Arguments
    Needs to summarize the current main arguments.
    """
    log_message("Evaluating Dimension 6: Mainstream Arguments")
    
    llm = get_llm()
    
    # Extract representative posts from each category
    arguments_text = ""
    for category, posts in sim_data["representative_posts"].items():
        if posts:
            arguments_text += f"\n[{category}]:\n"
            for p in posts[:5]:
                arguments_text += f"- {p['content'][:80]}\n"
    
    if not real_case_data:
        prompt = f"""Please summarize the mainstream arguments.

Representative Views:
{arguments_text}

Please return in JSON format:
{{
    "percentage": "number (0-100, indicating the diversity and quality of arguments)",
    "summary": "Summarize main arguments, e.g., 'Mainstream view: 60% think it violates privacy, 30% think it's convenient, 10% are undecided'",
    "reasoning": "Reasoning"
}}"""
        
        result = _parse_llm_json_response(llm.invoke(prompt).content)
        result["category"] = "simulation_only"
        return result
    
    else:
        sim_result = _parse_llm_json_response(llm.invoke(f"""Summarize the mainstream arguments of the simulation.
Views: {arguments_text[:1500]}
Return JSON: {{"percentage": "number", "summary": "Main arguments summary", "reasoning": "Reasoning"}}""").content)
        
        # Extract mainstream arguments information
        arguments_info = ""
        if real_case_data.get("situation_summaries"):
            for sit_sum in real_case_data["situation_summaries"]:
                if sit_sum.get("main_arguments"):
                    arguments_info += f"\nRound {sit_sum['round']}: {sit_sum['main_arguments']}"
        
        real_result = _parse_llm_json_response(llm.invoke(f"""Summarize the mainstream arguments of the real case.
Case: {real_case_data['title']}
Background: {real_case_data['background']}
{'[Mainstream Arguments by Round]' + arguments_info if arguments_info else ''}
Outcome: {real_case_data['outcome']}
Return JSON: {{"percentage": "number (0-100, indicates diversity and quality of arguments)", "summary": "Main arguments summary", "reasoning": "Reasoning"}}""").content)
        
        context_note = _get_similarity_context_note()
        criteria = _get_similarity_evaluation_criteria("Mainstream Arguments")
        
        similarity_result = _parse_llm_json_response(llm.invoke(f"""Compare argument similarity.

{context_note}

Simulation Arguments: {sim_result['summary']} ({sim_result['percentage']}%)
Real Case Arguments: {real_result['summary']} ({real_result['percentage']}%)

{criteria}

Evaluation Steps:
1. Compare the core arguments of supporters and opponents.
2. Judge the logic and direction of the arguments.
3. Give a similarity score based on the scoring criteria.

Return JSON: {{
    "similarity_percentage": "number (0-100, as per scoring criteria)", 
    "summary": "Summary, e.g., 'Similarity 80%, core arguments are highly consistent'", 
    "reasoning": "Explanation: 1) argument comparison 2) logical direction analysis 3) basis for scoring"
}}""").content)
        
        return {
            "category": "comparative",
            "simulation": sim_result,
            "real_case": real_result,
            "similarity": similarity_result
        }


def evaluate_emotion_tone(sim_data: Dict, real_case_data: Dict = None) -> Dict[str, Any]:
    """
    Dimension 7: Emotional Tone
    """
    log_message("Evaluating Dimension 7: Emotional Tone")
    
    llm = get_llm()
    
    emotion_posts = ""
    for category, posts in sim_data["representative_posts"].items():
        for p in posts[:3]:
            emotion_posts += f"[{category}] {p['content'][:60]}\n"
    
    if not real_case_data:
        prompt = f"""Evaluate the emotional tone.

Post Samples:
{emotion_posts}

Return in JSON format:
{{
    "percentage": "number (0-100, indicating emotional intensity, 50=peaceful, >50=intense, <50=apathetic)",
    "summary": "e.g., 'Emotional tone 75%, overall angry and questioning'",
    "reasoning": "Reasoning"
}}"""
        
        result = _parse_llm_json_response(llm.invoke(prompt).content)
        result["category"] = "simulation_only"
        return result
    
    else:
        sim_result = _parse_llm_json_response(llm.invoke(f"""Evaluate simulation emotion.
Posts: {emotion_posts[:1000]}
Return JSON: {{"percentage": "number", "summary": "Summary", "reasoning": "Reasoning"}}""").content)
        
        # Extract emotional tone information
        emotion_info = ""
        if real_case_data.get("situation_summaries"):
            for sit_sum in real_case_data["situation_summaries"]:
                if sit_sum.get("emotional_tone"):
                    emotion_info += f"\nRound {sit_sum['round']}: {sit_sum['emotional_tone']}"
        
        real_result = _parse_llm_json_response(llm.invoke(f"""Evaluate real case emotion.
Case: {real_case_data['title']}
{'[Emotional Tone by Round]' + emotion_info if emotion_info else ''}
Media Coverage: {real_case_data['outcome']['metrics']}
Return JSON: {{"percentage": "number (0-100, 50=peaceful, >50=intense, <50=apathetic)", "summary": "Summary", "reasoning": "Reasoning"}}""").content)
        
        context_note = _get_similarity_context_note()
        criteria = _get_similarity_evaluation_criteria("Emotional Tone")
        
        similarity_result = _parse_llm_json_response(llm.invoke(f"""Compare emotion similarity.

{context_note}

Simulation: {sim_result['summary']} ({sim_result['percentage']}%)
Real Case: {real_result['summary']} ({real_result['percentage']}%)

{criteria}

Evaluation Steps:
1. Compare the dominant emotion types.
2. Judge the level of emotional intensity.
3. Give a similarity score based on the scoring criteria.

Return JSON: {{
    "similarity_percentage": "number (0-100, as per scoring criteria)", 
    "summary": "Summary, e.g., 'Similarity 78%, both show angry emotions'", 
    "reasoning": "Explanation: 1) emotion type comparison 2) intensity comparison 3) basis for scoring"
}}""").content)
        
        return {
            "category": "comparative",
            "simulation": sim_result,
            "real_case": real_result,
            "similarity": similarity_result
        }


def evaluate_pr_response_pattern(sim_data: Dict, real_case_data: Dict = None) -> Dict[str, Any]:
    """
    Dimension 8: PR Strategy Response Pattern
    """
    log_message("Evaluating Dimension 8: PR Strategy Response Pattern")
    
    llm = get_llm()
    
    # Analyze reactions after each PR round
    response_info = ""
    for round_num in sorted(sim_data["stance_by_round"].keys()):
        if round_num > 0:
            stats = sim_data["stance_by_round"][round_num]
            response_info += f"After Round {round_num} PR: Support {stats['positive_count']}, Oppose {stats['negative_count']}, Neutral {stats['neutral_count']}\n"
    
    if not real_case_data:
        prompt = f"""Evaluate the response to the PR strategies.

Response by Round:
{response_info}

Strategies:
{json.dumps(sim_data['pr_strategies'], ensure_ascii=False)}

Return in JSON format:
{{
    "percentage": "number (0-100, indicating strategy effectiveness)",
    "summary": "e.g., 'Strategy response 55%, effect is average, partially accepted and partially questioned'",
    "reasoning": "Reasoning"
}}"""
        
        result = _parse_llm_json_response(llm.invoke(prompt).content)
        result["category"] = "simulation_only"
        return result
    
    else:
        sim_result = _parse_llm_json_response(llm.invoke(f"""Evaluate simulation response.
Response: {response_info}
Return JSON: {{"percentage": "number", "summary": "Summary", "reasoning": "Reasoning"}}""").content)
        
        strategies_text = "\n".join([f"Round {s['round']}: {s['content'][:50]}..." for s in real_case_data['strategies']])
        real_result = _parse_llm_json_response(llm.invoke(f"""Evaluate real case response.
Strategies: {strategies_text}
Outcome: {real_case_data['outcome']}
Return JSON: {{"percentage": "number", "summary": "Summary", "reasoning": "Reasoning"}}""").content)
        
        context_note = _get_similarity_context_note()
        criteria = _get_similarity_evaluation_criteria("PR Strategy Response Pattern")
        
        similarity_result = _parse_llm_json_response(llm.invoke(f"""Compare response pattern similarity.

{context_note}

Simulation: {sim_result['summary']} ({sim_result['percentage']}%)
Real Case: {real_result['summary']} ({real_result['percentage']}%)

{criteria}

Evaluation Steps:
1. Compare public acceptance of the strategy.
2. Judge if the direction of the strategy's effect is consistent.
3. Give a similarity score based on the scoring criteria.

Return JSON: {{
    "similarity_percentage": "number (0-100, as per scoring criteria)", 
    "summary": "Summary, e.g., 'Similarity 73%, response patterns are basically consistent'", 
    "reasoning": "Explanation: 1) acceptance comparison 2) effect direction analysis 3) basis for scoring"
}}""").content)
        
        return {
            "category": "comparative",
            "simulation": sim_result,
            "real_case": real_result,
            "similarity": similarity_result
        }


def evaluate_topic_diffusion(sim_data: Dict, real_case_data: Dict = None) -> Dict[str, Any]:
    """
    Dimension 9: Secondary Topic Diffusion Path
    """
    log_message("Evaluating Dimension 9: Secondary Topic Diffusion Path")
    
    llm = get_llm()
    
    history_sample = sim_data["formatted_history"][:2000]
    
    if not real_case_data:
        prompt = f"""Evaluate topic diffusion.

Initial Topic: {sim_data['initial_topic']}

Discussion History:
{history_sample}

Return in JSON format:
{{
    "percentage": "number (0-100, indicating diffusion richness)",
    "summary": "e.g., 'Topic diffusion 60%, from the initial issue to a discussion of corporate culture'",
    "reasoning": "Reasoning"
}}"""
        
        result = _parse_llm_json_response(llm.invoke(prompt).content)
        result["category"] = "simulation_only"
        return result
    
    else:
        sim_result = _parse_llm_json_response(llm.invoke(f"""Evaluate simulation diffusion.
Initial: {sim_data['initial_topic']}
Discussion: {history_sample[:1000]}
Return JSON: {{"percentage": "number", "summary": "Summary", "reasoning": "Reasoning"}}""").content)
        
        # Extract topic diffusion information
        diffusion_info = ""
        if real_case_data.get("situation_summaries"):
            for sit_sum in real_case_data["situation_summaries"]:
                if sit_sum.get("secondary_topic_diffusion"):
                    diffusion_info += f"\nRound {sit_sum['round']}: {sit_sum['secondary_topic_diffusion']}"
        
        real_result = _parse_llm_json_response(llm.invoke(f"""Evaluate real case diffusion.
Case: {real_case_data['title']}
Background: {real_case_data['background']}
{'[Topic Diffusion by Round]' + diffusion_info if diffusion_info else ''}
Return JSON: {{"percentage": "number (0-100, indicates diffusion richness)", "summary": "Summary", "reasoning": "Reasoning"}}""").content)
        
        context_note = _get_similarity_context_note()
        criteria = _get_similarity_evaluation_criteria("Secondary Topic Diffusion Path")
        
        similarity_result = _parse_llm_json_response(llm.invoke(f"""Compare diffusion path similarity.

{context_note}

Simulation: {sim_result['summary']} ({sim_result['percentage']}%)
Real Case: {real_result['summary']} ({real_result['percentage']}%)

{criteria}

Evaluation Steps:
1. Compare the generation of secondary topics.
2. Judge if the diffusion direction is consistent.
3. Give a similarity score based on the scoring criteria.

Return JSON: {{
    "similarity_percentage": "number (0-100, as per scoring criteria)", 
    "summary": "Summary, e.g., 'Similarity 70%, diffusion paths are basically consistent'", 
    "reasoning": "Explanation: 1) secondary topic comparison 2) diffusion direction analysis 3) basis for scoring"
}}""").content)
        
        return {
            "category": "comparative",
            "simulation": sim_result,
            "real_case": real_result,
            "similarity": similarity_result
        }


# ============================================================================
# Comprehensive Evaluation Function
# ============================================================================

def comprehensive_evaluation(sim_data: Dict, real_case_data: Dict = None) -> Dict[str, Any]:
    """
    Executes a full 9-dimension evaluation.
    
    Args:
        sim_data: Prepared simulation data.
        real_case_data: Prepared real case data (optional).
    
    Returns:
        A complete evaluation report.
    """
    log_message("\n" + "="*80)
    log_message("=== Starting Comprehensive Evaluation ===")
    log_message("="*80)
    
    # Add evaluation context note for comparative evaluation
    evaluation_context = None
    if real_case_data:
        sim_rounds = sim_data["total_rounds"]
        real_rounds = real_case_data["total_rounds"]
        evaluation_context = {
            "simulation_rounds": sim_rounds,
            "real_case_rounds": real_rounds,
            "is_partial_simulation": sim_rounds < real_rounds,
            "context_note": f"Note: The simulation ran for {sim_rounds} rounds, while the real case has {real_rounds} rounds. The simulation is a partial replication of the real case, and the evaluation should focus on the similarity of the executed rounds, not on completeness."
        }
        log_message(f"\nEvaluation Context: {evaluation_context['context_note']}")
    
    # Define evaluation functions and weights
    dimensions = [
        ("Overall Stance Tendency", evaluate_stance_tendency, 0.12),
        ("Public Opinion Evolution Direction", evaluate_evolution_direction, 0.12),
        ("Degree of Public Opinion Polarization", evaluate_polarization_degree, 0.08),
        ("Timing of Key Turning Points", evaluate_turning_point_timing, 0.08),
        ("Core Issue Focus", evaluate_core_issue_focus, 0.15),
        ("Mainstream Arguments", evaluate_mainstream_arguments, 0.12),
        ("Emotional Tone", evaluate_emotion_tone, 0.10),
        ("PR Strategy Response Pattern", evaluate_pr_response_pattern, 0.10),
        ("Secondary Topic Diffusion Path", evaluate_topic_diffusion, 0.13)
    ]
    
    results = {}
    is_comparative = real_case_data is not None
    
    for dim_name, eval_func, weight in dimensions:
        log_message(f"\n--- Evaluating: {dim_name} (Weight: {weight}) ---")
        try:
            result = eval_func(sim_data, real_case_data)
            results[dim_name] = {
                "weight": weight,
                "details": result
            }
            
            # Print evaluation result
            if result["category"] == "simulation_only":
                log_message(f"✓ {dim_name}: {result['percentage']}% - {result['summary']}")
            else:  # comparative
                log_message(f"✓ Simulation: {result['simulation']['percentage']}% - {result['simulation']['summary']}")
                log_message(f"✓ Real Case: {result['real_case']['percentage']}% - {result['real_case']['summary']}")
                log_message(f"✓ Similarity: {result['similarity']['similarity_percentage']}% - {result['similarity']['summary']}")
            
        except Exception as e:
            log_message(f"✗ {dim_name} evaluation failed: {str(e)}")
            results[dim_name] = {
                "weight": weight,
                "details": {
                    "category": "error",
                    "error": str(e)
                }
            }
    
    # Generate summary
    if is_comparative:
        # Scenario 2: Calculate overall similarity
        similarity_scores = []
        for dim_name, dim_data in results.items():
            details = dim_data["details"]
            if details.get("category") == "comparative" and "similarity" in details:
                similarity_scores.append(details["similarity"].get("similarity_percentage", 50))
        
        overall_similarity = sum(similarity_scores) / len(similarity_scores) if similarity_scores else 50
        
        summary = f"""
[Scenario 2: Similarity Comparison Evaluation]
Overall Similarity: {overall_similarity:.1f}%

Evaluation Note:
- Each dimension was evaluated for both the simulation and the real case.
- The LLM then judged the degree of similarity.
- The overall similarity is the average of the similarity scores of each dimension.
        """.strip()
        
        return {
            "evaluation_type": "comparative",
            "overall_similarity_percentage": round(overall_similarity, 1),
            "dimension_scores": results,
            "summary": summary
        }
    else:
        # Scenario 1: Summarize percentages for each dimension
        summary = "[Scenario 1: Simulation Quality Evaluation]\n\n"
        for dim_name, dim_data in results.items():
            details = dim_data["details"]
            if details.get("category") == "simulation_only":
                summary += f"{dim_name}: {details.get('percentage', 0)}% - {details.get('summary', '')}\n"
        
        return {
            "evaluation_type": "standalone",
            "dimension_scores": results,
            "summary": summary
        }
