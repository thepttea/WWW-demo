"""
æµ‹è¯•è¯„ä¼°ç³»ç»ŸAPIçš„è„šæœ¬
ä½¿ç”¨FastAPIçš„TestClientè¿›è¡Œå•å…ƒæµ‹è¯•

ç”¨æ³•:
  python test_evaluation_api.py          # æµ‹è¯•æ‰€æœ‰åœºæ™¯
  python test_evaluation_api.py 1        # åªæµ‹è¯•åœºæ™¯1
  python test_evaluation_api.py 2        # åªæµ‹è¯•åœºæ™¯2
"""

from fastapi.testclient import TestClient
import sys
import os
import argparse

# æ·»åŠ codeç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'code'))

from api_server import app

client = TestClient(app)

def test_scenario1_complete_flow():
    """æµ‹è¯•åœºæ™¯ä¸€çš„å®Œæ•´æµç¨‹"""
    print("\n" + "="*80)
    print("æµ‹è¯•åœºæ™¯ä¸€ï¼šè‡ªç”±æ¨¡æ‹Ÿ + è´¨é‡è¯„ä¼°")
    print("="*80)
    
    # 1. å¯åŠ¨æ¨¡æ‹Ÿ
    print("\n1. å¯åŠ¨åœºæ™¯ä¸€æ¨¡æ‹Ÿ...")
    response = client.post("/api/scenario1/simulation/start", json={
        "initialTopic": "æŸç§‘æŠ€å…¬å¸æ¨å‡ºæ–°åŠŸèƒ½ï¼Œé»˜è®¤å¼€å¯ç”¨æˆ·ä½ç½®å…±äº«ï¼Œå¼•å‘éšç§æ‹…å¿§",
        "llmModel": "gpt-4",
        "simulationConfig": {
            "agents": 10,  # æµ‹è¯•ç”¨å°‘é‡agent
            "num_rounds": 1,
            "interactionProbability": 0.8
        },
        "prStrategy": "æˆ‘ä»¬å¯¹é€ æˆçš„å›°æ‰°æ·±è¡¨æ­‰æ„ï¼Œå·²ç«‹å³å…³é—­è¯¥åŠŸèƒ½ï¼Œå°†è¿›è¡Œå…¨é¢å®¡æŸ¥ã€‚"
    })
    
    assert response.status_code == 200, f"å¯åŠ¨å¤±è´¥: {response.text}"
    data = response.json()
    assert data["success"] == True
    
    simulation_id = data["data"]["simulationId"]
    print(f"âœ“ æ¨¡æ‹Ÿå·²å¯åŠ¨: {simulation_id}")
    print(f"  çŠ¶æ€: {data['data']['status']}")
    
    # 2. è·å–æ¨¡æ‹Ÿç»“æœï¼ˆåªè¿è¡Œä¸€è½®ï¼Œè·³è¿‡ç¬¬äºŒè½®ç­–ç•¥ï¼‰
    print("\n3. è·å–æ¨¡æ‹Ÿç»“æœ...")
    response = client.get(f"/api/scenario1/simulation/{simulation_id}/result")
    
    assert response.status_code == 200
    result = response.json()["data"]
    print(f"âœ“ å½“å‰è½®æ¬¡: {result['round']}")
    print(f"  æ€»agentæ•°: {result['summary']['totalAgents']}")
    print(f"  æ´»è·ƒagent: {result['summary']['activeAgents']}")
    print(f"  æ€»å¸–å­æ•°: {result['summary']['totalPosts']}")
    print(f"  æ­£é¢æƒ…ç»ª: {result['summary']['positiveSentiment']:.2%}")
    print(f"  è´Ÿé¢æƒ…ç»ª: {result['summary']['negativeSentiment']:.2%}")
    
    # 4. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
    print("\n4. ç”Ÿæˆåœºæ™¯ä¸€è¯„ä¼°æŠ¥å‘Šï¼ˆåŒ…å«9ç»´åº¦è¯„ä¼°ï¼‰...")
    response = client.post("/api/scenario1/reports/generate", json={
        "simulationId": simulation_id,
        "reportType": "comprehensive"
    })
    
    assert response.status_code == 200
    report = response.json()["data"]
    
    print(f"âœ“ æŠ¥å‘Šå·²ç”Ÿæˆ: {report['reportId']}")
    print(f"  æŠ¥å‘Šç±»å‹: {report['reportType']}")
    
    # æ£€æŸ¥è¯„ä¼°ç»“æœ
    evaluation = report["evaluation"]
    print(f"\nã€è´¨é‡è¯„ä¼°ç»“æœã€‘")
    print(f"  è¯„ä¼°ç±»å‹: {evaluation['evaluation_type']}")
    
    # æ˜¾ç¤ºæ€»ä½“è¾¾æ ‡åº¦ï¼ˆæ–°å¢ï¼‰
    if 'overall_ideal_achievement_percentage' in evaluation:
        print(f"  æ€»ä½“è¾¾æ ‡åº¦: {evaluation['overall_ideal_achievement_percentage']:.1f}åˆ†")
        print(f"  è¯„çº§: {evaluation.get('rating', 'N/A')}")
    
    print(f"\nã€å„ç»´åº¦è¯„åˆ†ã€‘")
    for dim_name, dim_data in evaluation['dimension_scores'].items():
        details = dim_data['details']
        # å…¼å®¹å¤šç§è¿”å›æ ¼å¼
        if 'ideal_achievement_percentage' in details:
            # æ–°æ ¼å¼ï¼šåŒ…å«é‡åŒ–åˆ†æ•°
            score = details['ideal_achievement_percentage']
            desc = details.get('description', '')
            print(f"  {dim_name}: {score:.1f}åˆ† (æƒé‡: {dim_data['weight']})")
            print(f"    æè¿°: {desc[:80]}...")
        elif 'percentage' in details:
            # æ—§æ ¼å¼1
            print(f"  {dim_name}: {details['percentage']}% (æƒé‡: {dim_data['weight']})")
            print(f"    æ€»ç»“: {details['summary'][:80]}...")
        else:
            # æ—§æ ¼å¼2
            desc = details.get('description', '')
            print(f"  {dim_name} (æƒé‡: {dim_data['weight']})")
            print(f"    æè¿°: {desc[:80]}...")
    
    print(f"\nã€è¯„ä¼°æ€»ç»“ã€‘")
    print(evaluation['summary'][:300] + "...")
    
    # éªŒè¯æŠ¥å‘Šç»“æ„
    assert report['reportType'] == 'scenario1', f"æŠ¥å‘Šç±»å‹é”™è¯¯: {report['reportType']}"
    assert evaluation['evaluation_type'] == 'standalone', f"è¯„ä¼°ç±»å‹é”™è¯¯: {evaluation['evaluation_type']}"
    assert len(evaluation['dimension_scores']) == 9, f"ç»´åº¦æ•°é‡é”™è¯¯: {len(evaluation['dimension_scores'])}"
    
    # éªŒè¯æ–°å¢çš„é¡¶å±‚å­—æ®µ
    assert 'overall_ideal_achievement_percentage' in evaluation, "ç¼ºå°‘ overall_ideal_achievement_percentage"
    assert 'rating' in evaluation, "ç¼ºå°‘ rating"
    overall_score = evaluation['overall_ideal_achievement_percentage']
    assert 0 <= overall_score <= 100, f"æ€»ä½“è¾¾æ ‡åº¦è¶…å‡ºèŒƒå›´: {overall_score}"
    assert isinstance(evaluation['rating'], str), "ratingåº”è¯¥æ˜¯å­—ç¬¦ä¸²"
    
    # éªŒè¯æ¯ä¸ªç»´åº¦çš„ç»“æ„
    for dim_name, dim_data in evaluation['dimension_scores'].items():
        assert 'weight' in dim_data, f"{dim_name} ç¼ºå°‘ weight"
        details = dim_data['details']
        assert details.get('category') == 'simulation_only', f"{dim_name} categoryé”™è¯¯: {details.get('category')}"
        
        # éªŒè¯æ–°æ ¼å¼ï¼šideal_achievement_percentage + description + key_features + reasoning
        assert 'ideal_achievement_percentage' in details, f"{dim_name} ç¼ºå°‘ ideal_achievement_percentage"
        assert 'description' in details, f"{dim_name} ç¼ºå°‘ description"
        assert 'key_features' in details, f"{dim_name} ç¼ºå°‘ key_features"
        assert 'reasoning' in details, f"{dim_name} ç¼ºå°‘ reasoning"
        
        # éªŒè¯åˆ†æ•°èŒƒå›´
        score = details['ideal_achievement_percentage']
        assert 0 <= score <= 100, f"{dim_name} è¾¾æ ‡åº¦è¶…å‡ºèŒƒå›´: {score}"
        
        # éªŒè¯key_featuresæ˜¯åˆ—è¡¨
        assert isinstance(details['key_features'], list), f"{dim_name} key_featuresåº”è¯¥æ˜¯åˆ—è¡¨"
    
    print("\nâœ… åœºæ™¯ä¸€æµ‹è¯•é€šè¿‡ï¼")
    return simulation_id


def select_test_case():
    """è®©ç”¨æˆ·é€‰æ‹©è¦æµ‹è¯•çš„æ¡ˆä¾‹"""
    # æ¨èçš„3ä¸ªå…¸å‹æ¡ˆä¾‹
    recommended_cases = {
        '1': {
            'id': 'CASE-02',
            'name': 'Apple iPad Pro å¹¿å‘Šäº‰è®®',
            'reason': 'è¥é”€ä¸ç¯å¢ƒä¸åŒ¹é…ï¼Œå¿«é€Ÿé“æ­‰æˆåŠŸåŒ–è§£å±æœº'
        },
        '2': {
            'id': 'CASE-03',
            'name': 'è¥¿è´èœé¢æ‘é¢„åˆ¶èœäº‰è®®',
            'reason': 'ä»·å€¼è¡ŒåŠ¨èƒŒç¦»ï¼Œå…ˆå¼ºç¡¬åé“æ­‰çš„å¯¹æ¯”æ¡ˆä¾‹'
        },
        '3': {
            'id': 'CASE-04',
            'name': 'å†œå¤«å±±æ³‰èˆ†æƒ…é£æš´',
            'reason': 'å¤æ‚èˆ†æƒ…æ¼”å˜ï¼Œå¤šè½®æ¬¡åº”å¯¹ç­–ç•¥ï¼ˆ4è½®ï¼‰'
        }
    }
    
    print("\n" + "="*80)
    print("è¯·é€‰æ‹©è¦æµ‹è¯•çš„æ¡ˆä¾‹")
    print("="*80)
    print("\nã€æ¨èçš„å…¸å‹æ¡ˆä¾‹ã€‘\n")
    
    for key, case_info in recommended_cases.items():
        print(f"{key}. {case_info['name']} ({case_info['id']})")
        print(f"   ç‰¹ç‚¹: {case_info['reason']}\n")
    
    print("0. æ˜¾ç¤ºæ‰€æœ‰æ¡ˆä¾‹")
    print()
    
    while True:
        choice = input("è¯·è¾“å…¥é€‰æ‹© (0-3ï¼Œæˆ–æŒ‰ Enter ä½¿ç”¨é»˜è®¤æ¡ˆä¾‹2): ").strip()
        
        # é»˜è®¤é€‰æ‹©æ¡ˆä¾‹2ï¼ˆè¥¿è´ï¼‰
        if choice == "":
            choice = "2"
        
        if choice in ['1', '2', '3']:
            selected_case_id = recommended_cases[choice]['id']
            print(f"\nâœ“ å·²é€‰æ‹©: {recommended_cases[choice]['name']}")
            return selected_case_id
        elif choice == '0':
            # æ˜¾ç¤ºæ‰€æœ‰æ¡ˆä¾‹
            return show_all_cases()
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 0-3")


def show_all_cases():
    """æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨æ¡ˆä¾‹å¹¶è®©ç”¨æˆ·é€‰æ‹©"""
    print("\n" + "="*80)
    print("è·å–æ‰€æœ‰å¯ç”¨æ¡ˆä¾‹...")
    print("="*80)
    
    response = client.get("/api/scenario2/cases")
    assert response.status_code == 200
    cases = response.json()["data"]
    
    print(f"\nâœ“ å…±æœ‰ {len(cases)} ä¸ªæ¡ˆä¾‹\n")
    
    for idx, case in enumerate(cases, 1):
        print(f"{idx}. {case['title']} ({case['id']})")
        print(f"   å…¬å¸: {case.get('company', 'N/A')} | è½®æ¬¡: {case['totalRounds']}")
    
    print()
    
    while True:
        choice = input(f"è¯·è¾“å…¥æ¡ˆä¾‹ç¼–å· (1-{len(cases)}): ").strip()
        try:
            idx = int(choice)
            if 1 <= idx <= len(cases):
                selected_case = cases[idx - 1]
                print(f"\nâœ“ å·²é€‰æ‹©: {selected_case['title']}")
                return selected_case['id']
            else:
                print(f"âŒ è¯·è¾“å…¥ 1-{len(cases)} ä¹‹é—´çš„æ•°å­—")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")


def test_scenario2_complete_flow(case_id=None):
    """æµ‹è¯•åœºæ™¯äºŒçš„å®Œæ•´æµç¨‹"""
    print("\n" + "="*80)
    print("æµ‹è¯•åœºæ™¯äºŒï¼šçœŸå®æ¡ˆä¾‹æ¨¡æ‹Ÿ + ç›¸ä¼¼åº¦å¯¹æ¯”")
    print("="*80)
    
    # 1. è·å–æ¡ˆä¾‹åˆ—è¡¨
    print("\n1. è·å–å¯ç”¨æ¡ˆä¾‹åˆ—è¡¨...")
    response = client.get("/api/scenario2/cases")
    
    assert response.status_code == 200
    cases = response.json()["data"]
    print(f"âœ“ å…±æœ‰ {len(cases)} ä¸ªæ¡ˆä¾‹å¯ç”¨")
    
    # æŸ¥æ‰¾æŒ‡å®šçš„æ¡ˆä¾‹
    test_case = None
    if case_id:
        for case in cases:
            if case["id"] == case_id:
                test_case = case
                break
    
    if not test_case:
        print(f"  âš ï¸ æœªæ‰¾åˆ°æŒ‡å®šæ¡ˆä¾‹ {case_id}ï¼Œä½¿ç”¨é»˜è®¤æ¡ˆä¾‹")
        # é»˜è®¤ä½¿ç”¨CASE-03 (Xibeié¢„åˆ¶èœæ¡ˆä¾‹)
        for case in cases:
            if case["id"] in ["CASE-03", "case_009"]:
                test_case = case
                break
    
    if not test_case:
        print("  âš ï¸ æœªæ‰¾åˆ°é»˜è®¤æ¡ˆä¾‹ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ¡ˆä¾‹")
        test_case = cases[0]
    
    print(f"  é€‰æ‹©æ¡ˆä¾‹: {test_case['title']}")
    print(f"  æ¡ˆä¾‹ID: {test_case['id']}")
    print(f"  æ€»è½®æ¬¡: {test_case['totalRounds']}")
    
    # 2. è·å–æ¡ˆä¾‹è¯¦æƒ…
    print(f"\n2. è·å–æ¡ˆä¾‹è¯¦æƒ…...")
    response = client.get(f"/api/scenario2/cases/{test_case['id']}")
    
    assert response.status_code == 200
    case_detail = response.json()["data"]
    print(f"âœ“ æ¡ˆä¾‹èƒŒæ™¯: {case_detail['background'][:100]}...")
    
    # 3. å¯åŠ¨åœºæ™¯äºŒæ¨¡æ‹Ÿ
    print("\n3. å¯åŠ¨åœºæ™¯äºŒæ¨¡æ‹Ÿ...")
    response = client.post("/api/scenario2/simulation/start", json={
        "caseId": test_case['id'],
        "llmModel": "gpt-4",
        "simulationConfig": {
            "agents": 10,
            "num_rounds": 1,
            "interactionProbability": 0.8
        }
    })
    
    assert response.status_code == 200
    data = response.json()["data"]
    
    simulation_id = data["simulationId"]
    print(f"âœ“ æ¨¡æ‹Ÿå·²å¯åŠ¨: {simulation_id}")
    print(f"  å…³è”æ¡ˆä¾‹: {data['caseId']}")
    print(f"  å½“å‰è½®æ¬¡: {data['currentRound']}/{data['totalRounds']}")
    
    # 4. æ¨è¿›å‰©ä½™è½®æ¬¡
    current_round = data['currentRound']
    total_rounds = data['totalRounds']
    
    while current_round < total_rounds:
        print(f"\n4.{current_round} æ¨è¿›åˆ°ç¬¬ {current_round + 1} è½®...")
        response = client.post(f"/api/scenario2/simulation/{simulation_id}/next-round")
        
        assert response.status_code == 200
        data = response.json()["data"]
        current_round = data['currentRound']
        print(f"âœ“ ç¬¬ {current_round} è½®æ¨¡æ‹Ÿå®Œæˆ")
    
    # 5. è·å–æœ€ç»ˆç»“æœ
    print(f"\n5. è·å–æ¨¡æ‹Ÿæœ€ç»ˆç»“æœ...")
    response = client.get(f"/api/scenario2/simulation/{simulation_id}/result")
    
    assert response.status_code == 200
    result = response.json()["data"]
    print(f"âœ“ æ¨¡æ‹Ÿå·²å®Œæˆæ‰€æœ‰ {result['totalRounds']} è½®")
    print(f"  æ€»å¸–å­æ•°: {result['summary']['totalPosts']}")
    print(f"  æ­£é¢æƒ…ç»ª: {result['summary']['positiveSentiment']:.2%}")
    print(f"  è´Ÿé¢æƒ…ç»ª: {result['summary']['negativeSentiment']:.2%}")
    
    # 6. ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š
    print("\n6. ç”Ÿæˆåœºæ™¯äºŒå¯¹æ¯”åˆ†ææŠ¥å‘Šï¼ˆæ¨¡æ‹Ÿ vs çœŸå®æ¡ˆä¾‹ï¼‰...")
    response = client.post("/api/scenario2/reports/generate", json={
        "simulationId": simulation_id,
        "reportType": "comprehensive"
    })
    
    assert response.status_code == 200
    report = response.json()["data"]
    
    print(f"âœ“ æŠ¥å‘Šå·²ç”Ÿæˆ: {report['reportId']}")
    print(f"  æŠ¥å‘Šç±»å‹: {report['reportType']}")
    print(f"  å¯¹æ¯”æ¡ˆä¾‹: {report['caseTitle']}")
    
    # æ£€æŸ¥è¯„ä¼°ç»“æœ
    evaluation = report["evaluation"]
    overall_similarity = report.get("overallSimilarityPercentage", 0)
    
    print(f"\nã€ç›¸ä¼¼åº¦è¯„ä¼°ç»“æœã€‘")
    print(f"  æ€»ä½“ç›¸ä¼¼åº¦: {overall_similarity:.1f}%")
    print(f"  è¯„ä¼°ç±»å‹: {evaluation['evaluation_type']}")
    
    print(f"\nã€å„ç»´åº¦å¯¹æ¯”è¯„åˆ†ã€‘")
    for dim_name, dim_data in evaluation['dimension_scores'].items():
        details = dim_data['details']
        if details.get('category') == 'comparative':
            sim = details['simulation']
            real = details['real_case']
            similarity = details['similarity']
            print(f"\n  {dim_name} (æƒé‡: {dim_data['weight']}):")
            sim_desc = sim.get('description', sim.get('summary', ''))
            real_desc = real.get('description', real.get('summary', ''))
            print(f"    æ¨¡æ‹Ÿ: {sim_desc[:80]}...")
            print(f"    çœŸå®: {real_desc[:80]}...")
            print(f"    ç›¸ä¼¼åº¦: {similarity['similarity_percentage']:.1f}%")
    
    print(f"\nã€ç›¸ä¼¼åº¦æ€»ç»“ã€‘")
    print(evaluation['summary'])
    
    # éªŒè¯æŠ¥å‘Šç»“æ„
    assert report['reportType'] == 'scenario2_comparative'
    assert 'caseId' in report
    assert 'caseTitle' in report
    assert 'overallSimilarityPercentage' in report
    assert evaluation['evaluation_type'] == 'comparative'
    assert 0 <= overall_similarity <= 100
    assert len(evaluation['dimension_scores']) == 9
    
    # éªŒè¯æ¯ä¸ªç»´åº¦éƒ½æœ‰å¯¹æ¯”æ•°æ®
    for dim_name, dim_data in evaluation['dimension_scores'].items():
        details = dim_data['details']
        assert details.get('category') == 'comparative', f"{dim_name} ä¸æ˜¯å¯¹æ¯”ç±»å‹"
        assert 'simulation' in details, f"{dim_name} ç¼ºå°‘æ¨¡æ‹Ÿæ•°æ®"
        assert 'real_case' in details, f"{dim_name} ç¼ºå°‘çœŸå®æ¡ˆä¾‹æ•°æ®"
        assert 'similarity' in details, f"{dim_name} ç¼ºå°‘ç›¸ä¼¼åº¦æ•°æ®"
    
    print("\nâœ… åœºæ™¯äºŒæµ‹è¯•é€šè¿‡ï¼")
    return simulation_id


def test_report_differences():
    """éªŒè¯ä¸¤ä¸ªåœºæ™¯çš„æŠ¥å‘Šç¡®å®ä¸åŒ"""
    print("\n" + "="*80)
    print("éªŒè¯ï¼šä¸¤ä¸ªåœºæ™¯çš„æŠ¥å‘Šç»“æ„å’Œå†…å®¹ä¸åŒ")
    print("="*80)
    
    print("\nã€åœºæ™¯ä¸€æŠ¥å‘Šç‰¹å¾ã€‘")
    print("  âœ“ reportType: 'scenario1'")
    print("  âœ“ evaluation_type: 'standalone'")
    print("  âœ“ è¯„ä¼°ç›®æ ‡: æ¨¡æ‹Ÿè´¨é‡ï¼ˆç™¾åˆ†æ¯”è¯„åˆ†ï¼‰")
    print("  âœ“ ç»´åº¦detailsåŒ…å«: percentage, summary, reasoning")
    print("  âœ“ æ—  caseId, caseTitle, overallSimilarityPercentage å­—æ®µ")
    print("  âœ“ æ¯ä¸ªç»´åº¦category='simulation_only'")
    
    print("\nã€åœºæ™¯äºŒæŠ¥å‘Šç‰¹å¾ã€‘")
    print("  âœ“ reportType: 'scenario2_comparative'")
    print("  âœ“ evaluation_type: 'comparative'")
    print("  âœ“ è¯„ä¼°ç›®æ ‡: ä¸çœŸå®æ¡ˆä¾‹çš„ç›¸ä¼¼åº¦ï¼ˆç™¾åˆ†æ¯”è¯„åˆ†ï¼‰")
    print("  âœ“ ç»´åº¦detailsåŒ…å«: simulation, real_case, similarity")
    print("  âœ“ æ¯ä¸ªéƒ¨åˆ†éƒ½æœ‰: percentage, summary, reasoning")
    print("  âœ“ æœ‰ caseId, caseTitle, overallSimilarityPercentage å­—æ®µ")
    print("  âœ“ æ¯ä¸ªç»´åº¦category='comparative'")
    
    print("\nã€æ–°è¯„ä¼°ç³»ç»Ÿç‰¹ç‚¹ã€‘")
    print("  âœ“ æ‰€æœ‰ç»´åº¦ä½¿ç”¨LLMè¯„ä¼°ï¼ˆä¸å†ä½¿ç”¨å…¬å¼ï¼‰")
    print("  âœ“ è¯„åˆ†æ”¹ä¸ºç™¾åˆ†æ¯”å±•ç¤ºï¼ˆ0-100%ï¼‰")
    print("  âœ“ æ¯ä¸ªç»´åº¦åŒ…å«ï¼šç™¾åˆ†æ¯”ã€æ€»ç»“ã€ç†ç”±")
    print("  âœ“ åœºæ™¯äºŒå…ˆåˆ†åˆ«è¯„ä¼°æ¨¡æ‹Ÿå’ŒçœŸå®ï¼Œå†LLMåˆ¤æ–­ç›¸ä¼¼åº¦")
    
    print("\nâœ… ä¸¤ä¸ªåœºæ™¯çš„æŠ¥å‘Šå®Œå…¨ç‹¬ç«‹ï¼Œè¯„ä¼°ç›®æ ‡ä¸åŒï¼")


def main():
    """è¿è¡Œæµ‹è¯•"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='æµ‹è¯•è¯„ä¼°ç³»ç»ŸAPI')
    parser.add_argument('scenario', nargs='?', choices=['1', '2', 'all'], default='all',
                       help='é€‰æ‹©è¦æµ‹è¯•çš„åœºæ™¯: 1=åœºæ™¯ä¸€, 2=åœºæ™¯äºŒ, all=å…¨éƒ¨æµ‹è¯• (é»˜è®¤: all)')
    args = parser.parse_args()
    
    scenario = args.scenario
    
    print("="*80)
    if scenario == '1':
        print("å¼€å§‹æµ‹è¯•åœºæ™¯ä¸€")
    elif scenario == '2':
        print("å¼€å§‹æµ‹è¯•åœºæ™¯äºŒ")
    else:
        print("å¼€å§‹æµ‹è¯•è¯„ä¼°ç³»ç»ŸAPIï¼ˆå…¨éƒ¨åœºæ™¯ï¼‰")
    print("="*80)
    
    try:
        # æ ¹æ®é€‰æ‹©è¿è¡Œæµ‹è¯•
        if scenario == '1':
            # åªæµ‹è¯•åœºæ™¯ä¸€
            test_scenario1_complete_flow()
            print("\n" + "="*80)
            print("ğŸ‰ åœºæ™¯ä¸€æµ‹è¯•é€šè¿‡ï¼")
            print("="*80)
            
        elif scenario == '2':
            # åªæµ‹è¯•åœºæ™¯äºŒ - å…ˆè®©ç”¨æˆ·é€‰æ‹©æ¡ˆä¾‹
            selected_case_id = select_test_case()
            test_scenario2_complete_flow(case_id=selected_case_id)
            print("\n" + "="*80)
            print("ğŸ‰ åœºæ™¯äºŒæµ‹è¯•é€šè¿‡ï¼")
            print("="*80)
            
        else:
            # æµ‹è¯•æ‰€æœ‰åœºæ™¯
            test_scenario1_complete_flow()
            
            # åœºæ™¯äºŒæµ‹è¯•æ—¶è®©ç”¨æˆ·é€‰æ‹©æ¡ˆä¾‹
            print("\n" + "="*80)
            print("æ¥ä¸‹æ¥æµ‹è¯•åœºæ™¯äºŒ")
            print("="*80)
            selected_case_id = select_test_case()
            test_scenario2_complete_flow(case_id=selected_case_id)
            
            test_report_differences()
            
            print("\n" + "="*80)
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
            print("="*80)
            print("\nã€æ€»ç»“ã€‘")
            print("âœ… åœºæ™¯ä¸€åŠŸèƒ½å®Œæ•´ï¼šè‡ªç”±æ¨¡æ‹Ÿ + è´¨é‡è¯„ä¼°ï¼ˆç™¾åˆ†æ¯”ï¼‰")
            print("âœ… åœºæ™¯äºŒåŠŸèƒ½å®Œæ•´ï¼šçœŸå®æ¡ˆä¾‹æ¨¡æ‹Ÿ + ç›¸ä¼¼åº¦å¯¹æ¯”ï¼ˆç™¾åˆ†æ¯”ï¼‰")
            print("âœ… ä¸¤ä¸ªåœºæ™¯çš„æŠ¥å‘Šæ¥å£å®Œå…¨ç‹¬ç«‹")
            print("âœ… æŠ¥å‘Šç»“æ„å’Œè¯„ä¼°ç›®æ ‡å®Œå…¨ä¸åŒ")
            print("âœ… 9ä¸ªè¯„ä¼°ç»´åº¦å…¨éƒ¨ä½¿ç”¨LLMè¯„ä¼°")
            print("âœ… æ¯ä¸ªç»´åº¦åŒ…å«ï¼šç™¾åˆ†æ¯”ã€æ€»ç»“ã€ç†ç”±")
            print("âœ… åœºæ™¯äºŒåˆ†åˆ«è¯„ä¼°æ¨¡æ‹Ÿå’ŒçœŸå®ï¼Œå†åˆ¤æ–­ç›¸ä¼¼åº¦")
        
    except AssertionError as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        raise
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()

